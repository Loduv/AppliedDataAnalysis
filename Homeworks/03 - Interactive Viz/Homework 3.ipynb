{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import folium\n",
    "from pysal.esda.mapclassify import Natural_Breaks as nb\n",
    "\n",
    "# data paths\n",
    "TASK_1_DATA_PATH = './data/lfsa_urgan_1_Data.csv'\n",
    "AMSTAT_UNEMPLOYED_PATH = './data/Arbeitslosenquoten_task_2_1.csv'\n",
    "AMSTAT_JOBSEEKING_PATH = './data/TASK_2_demandeurs_emploi.csv'\n",
    "PATH_TASK_3_1 = './data/amstat_3ch.csv'\n",
    "PATH_TASK_3_2 = './data/task_3_2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: European unemployment rate (Eurostat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data from [Eurostat](http://ec.europa.eu/eurostat/data/database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database by themes/Population and social conditions/Labour market (Labour)/Employment and Unemployment(Labour Force survey)(employ)/LFS series - detailed annual survey results (LFSA)/ Total unemployment - LFS series(lfsa_unemp)/  Unemployment rates by sex, age and nationality (%) (lfsa_urgan)\n",
    "\n",
    "\n",
    "\n",
    "Age from 15 to 74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we got the data:\n",
    "- we used the data explorer, then 'Download' button, 'Change Selection' to keep only what we're interested in, then download data in CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importing and cleaning\n",
    "First, we import the unemployement rates from the Eurostat website in a dataframe. Since we are looking for recent statistics, we only keep the data from last year (2016). We also discard all columns except the ones we need (Country and unemployment rate). A few countries also need to be renamed, so that the countries' names are consistent between the eurostat DataFrame and the topojson file. We also drop some unwanted rows, such as the average rate for countries from the European Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TASK_1_DATA_PATH)\n",
    "df = df[df['TIME']==2016] # only keep data from last year\n",
    "df = df[['GEO', 'Value']] # The only information we need is the country and the unemployment rate\n",
    "df.rename(columns={'GEO': 'Country', 'Value' : 'Unemployment rate' }, inplace=True)\n",
    "df.index = (range(len(df))) # re-index the dataframe\n",
    "\n",
    "# drop unwanted rows, rename some countries to match with the topojson data\n",
    "df = df.replace(to_replace='Former Yugoslav Republic of Macedonia, the', value='The former Yugoslav Republic of Macedonia')\n",
    "df = df.replace(to_replace='Germany (until 1990 former territory of the FRG)', value='Germany')\n",
    "df = df.drop(index=[0,1, 2, 3, 4, 5])\n",
    "\n",
    "df.index = (range(len(df))) # re-index the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we import the topojson data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_geo_path = r'topojson/europe.topojson.json'\n",
    "geo_json_data = json.load(open(state_geo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we are missing some data. To do this, we extract the list of countries from the topojson file and check if some of these countries are not in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract the list of countries from the topojson\n",
    "countries = [country['properties']['NAME'] for country in geo_json_data['objects']['europe']['geometries']]\n",
    "# From this list, print each country that does not appear in the DataFrame\n",
    "missing_countries = set(countries).difference(set(df.Country.values))\n",
    "print(\"The data is missing for %d out of %d countries:\\n\" %(len((missing_countries)), len(countries)))\n",
    "print('\\n'.join(missing_countries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure that each countries from the DataFrame has been matched to a country in the topojson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not set(df.Country.values).difference(set(countries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't have the unemployment rate of some countries in topojson file, we need to delete them such that our choropleth plot does not get spoiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_keep = [country['properties']['NAME'] in df.Country.values \n",
    "                for country in geo_json_data['objects']['europe']['geometries']]\n",
    "geo_json_data['objects']['europe']['geometries'] = np.array(geo_json_data['objects']['europe']['geometries'])[index_to_keep].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique to have propoer breaks\n",
    "\n",
    "We decided to split our colours intervals according to the Natural Breaks (Jenks) Classification. This methods arranges each groupings so there is less variation in each class or shading. \n",
    "\n",
    "We used a k-mean algorithm for 5 classes (the max that the choropleth function allows) in 1 dimension in order to compute the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breaks(serie):\n",
    "    #k-mean in 1D\n",
    "    breaks = nb(serie.values, k = 5)\n",
    "    scales = [min(serie.values)]\n",
    "    for x in breaks.bins[0:len(breaks.bins)]:\n",
    "        scales.append(x+0.01)\n",
    "    return scales\n",
    "\n",
    "thresh = compute_breaks(df['Unemployment rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the choropleth map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map([53, 15],  tiles='cartodbpositron', zoom_start=4)\n",
    "#ignore_missing_countries = lambda x: {'fillOpacity':1,'fillColor':'YlOrRd' if x['properties']['NAME'] in df.Country.values else 'black'}\n",
    "#folium.TopoJson(open(state_geo_path), 'objects.europe', style_function =ignore_missing_countries).add_to(m)\n",
    "m.choropleth(\n",
    "    geo_data=geo_json_data,\n",
    "    name='choropleth',\n",
    "    data=df,\n",
    "    columns=['Country', 'Unemployment rate'],\n",
    "    key_on='feature.properties.NAME',\n",
    "    topojson='objects.europe',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Unemployment Rate (%)',\n",
    "    threshold_scale= thresh\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('http://localhost:8888/notebooks/ADA_HW/AppliedDataAnalysis/Homeworks/03%20-%20Interactive%20Viz/europe_unemployment_rate.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Unemployment rate in Switzerland\n",
    "\n",
    "The goal of task 2 is to compute and plot the unemployment rate for each canton in Switzerland. We got the data from Amstat's website. We will address two definitions of *unemployment rate*:\n",
    "- The first one is the ratio of **jobseeking** people over the total active population.\n",
    "- The second one is the ratio of **unemployed** people over the total active population.\n",
    "\n",
    "Note that we consider the total **active** population, and not the total population, as children and elders should obviously not be taken into account.\n",
    "\n",
    "We are interested in recent data, but we also want to tackle the seasonality problem (i.e. unemployment may be higher at some periods of the year). Thus, taking the average of all the monthly ratios of 2017 (January-September) seemed to be a good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2.1\n",
    "#### Compute the jobseeking ratio\n",
    "\n",
    "In this section we compute the unemployment ratio in Switzerland's cantons, including the people that have a job but are looking for a new one. In other words, we are computing the jobseeking ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import topojson data\n",
    "topo_ch = r'topojson/ch-cantons.topojson.json'\n",
    "topo_ch_data = json.load(open(topo_ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data\n",
    "First, we clean the data by getting rid of unwanted rows and columns. Read the comments for more information about the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ch = pd.read_csv(AMSTAT_JOBSEEKING_PATH)\n",
    "\n",
    "# init new dataframe for jobseekers ratio\n",
    "jobseekers_ratio_df = pd.DataFrame()\n",
    "\n",
    "# set cantons as index\n",
    "df_ch.set_index('Kanton', drop=True, inplace=True)\n",
    "\n",
    "# drop unwanted columns\n",
    "cols_to_drop = [i for i in range(len(df_ch.columns)-6, len(df_ch.columns))]\n",
    "cols_to_drop.append(0)\n",
    "df_ch.drop(df_ch.columns[cols_to_drop],axis=1, inplace=True)\n",
    "\n",
    "# cast values to float and drop non-numerical characters\n",
    "df_ch = df_ch.replace({\"'\":\"\"}, regex = True)\n",
    "df_ch.iloc[1:len(df_ch)] = df_ch.iloc[1:len(df_ch)].astype(float)\n",
    "\n",
    "#for each month, compute the percentage of unemployed people, including those that already have a job but are looking for a new one\n",
    "for i in range(0, len(df_ch.columns), 3):  \n",
    "    unemployed_ratio = df_ch.iloc[1:len(df_ch)-1,i]\n",
    "    unemployed = df_ch.iloc[1:len(df_ch)-1,i+1]\n",
    "    jobseekers = df_ch.iloc[1:len(df_ch)-1,i+2]\n",
    "    jobseekers_ratio = (jobseekers*unemployed_ratio)/unemployed\n",
    "    jobseekers_ratio_df[list(df_ch.columns.values)[i]] = jobseekers_ratio\n",
    "\n",
    "    \n",
    "# Now let's aggregate this dataframe to compute, for each canton, the average jobseeking rate from january to september 2017\n",
    "jobseekers_ratio_df['jobseeking rate'] = jobseekers_ratio_df.mean(axis=1)\n",
    "\n",
    "jobseekers_ratio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then tackle the problem of cantons names: Some names need to be translated so that the names match between the topojson and the data from Amstat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, extract the list of cantons from the topojson\n",
    "cantons_topo = [canton['properties']['name'] for canton in topo_ch_data['objects']['cantons']['geometries']]\n",
    "# From this list, print each canton that does not appear in the DataFrame\n",
    "missing_cantons = set(cantons_topo).difference(set(df_ch.index.values))\n",
    "print(\"Some names are not matching %d / %d\" %(len(missing_cantons), len(cantons_topo)))\n",
    "print('\\n'.join(missing_cantons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a few names for the cantons whose names are not matching between topojson and dataframe\n",
    "jobseekers_ratio_df = jobseekers_ratio_df.rename(index={'Genf':'Genève', 'Waadt':'Vaud', 'Neuenburg':'Neuchâtel', 'Graubünden':'Graubünden/Grigioni', 'Tessin':'Ticino', 'Bern':'Bern/Berne', 'Wallis':'Valais/Wallis','Freiburg':'Fribourg'})\n",
    "\n",
    "# assert that every name is matching\n",
    "assert(set(cantons_topo) == set(jobseekers_ratio_df.index.values))\n",
    "\n",
    "# we need a 'cantons' column to bind the data with folium\n",
    "jobseekers_ratio_df['cantons'] = jobseekers_ratio_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the average jobseeking rate from january to september 2017 for each canton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_jobseeking = folium.Map([46.9, 8.2],  tiles='cartodbpositron', zoom_start=8)\n",
    "map_jobseeking.choropleth(\n",
    "    geo_data=topo_ch_data,\n",
    "    name='choropleth',\n",
    "    data=jobseekers_ratio_df,\n",
    "    columns=['cantons', 'jobseeking rate'],\n",
    "    key_on='feature.properties.name',\n",
    "    topojson='objects.cantons',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Jobseeking Rate (%)',\n",
    "    threshold_scale=compute_breaks(jobseekers_ratio_df['jobseeking rate'])\n",
    ")\n",
    "map_jobseeking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2\n",
    "#### Compute the unemployment ratio\n",
    "\n",
    "In this section we compute the unemployment ratio in Switzerland's cantons, **excluding the people that have a job but are looking for a new one**. This seems to be a better alternative than what we did in 2.1, since the people looking for a new job are not considered as unemployed anymore. Note that this is how Amstat computes the unemployment rate by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ch = pd.read_csv(AMSTAT_UNEMPLOYED_PATH)\n",
    "\n",
    "# drop unwanted columns\n",
    "cols_to_drop = [i for i in range(1, len(df_ch.columns), 2)]\n",
    "cols_to_drop.append(20)\n",
    "cols_to_drop.append(22)\n",
    "df_ch.drop(df_ch.columns[cols_to_drop],axis=1, inplace=True)\n",
    "\n",
    "#drop unwanted row\n",
    "df_ch.drop(index=[0, len(df_ch)-1], axis=0, inplace=True)\n",
    "\n",
    "# set cantons as index\n",
    "df_ch.set_index('Kanton', drop=True, inplace=True)\n",
    "\n",
    "# cast dataframe values as float\n",
    "df_ch=df_ch.astype(float)\n",
    "\n",
    "# compute the average unemployment rate for each canton\n",
    "df_ch['unemployment rate'] = df_ch.mean(axis=1)\n",
    "df_ch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again, we just replace a few names so that the topojson matches the DataFrame, then we assert each canton has a matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a few names for the cantons whose names are not matching between topojson and dataframe\n",
    "df_ch = df_ch.rename(index={'Genf':'Genève', 'Waadt':'Vaud', 'Neuenburg':'Neuchâtel', 'Graubünden':'Graubünden/Grigioni', 'Tessin':'Ticino', 'Bern':'Bern/Berne', 'Wallis':'Valais/Wallis','Freiburg':'Fribourg'})\n",
    "\n",
    "# assert that every name is matching\n",
    "assert(set(cantons_topo) == set(df_ch.index.values))\n",
    "\n",
    "# we need a 'cantons' column to bind the data with folium\n",
    "df_ch['cantons'] = df_ch.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the average unemployment rate from january to september 2017 for each canton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_ch = folium.Map([46.9, 8.2],  tiles='cartodbpositron', zoom_start=8)\n",
    "map_ch.choropleth(\n",
    "    geo_data=topo_ch_data,\n",
    "    name='choropleth',\n",
    "    data=df_ch,\n",
    "    columns=['cantons', 'unemployment rate'],\n",
    "    key_on='feature.properties.name',\n",
    "    topojson='objects.cantons',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Unemployment Rate (%)',\n",
    "    threshold_scale=compute_breaks(df_ch['unemployment rate'])\n",
    ")\n",
    "map_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret these differences ?\n",
    "\n",
    "To interpret the differences, we will compute the *variation rate* between the jobseeking rate and the unemployment rate. It is defined by :\n",
    "\n",
    "$$\\dfrac{\\text{jobseeking rate} - \\text{unemployment rate}}{\\text{unemployment rate}}$$\n",
    "\n",
    "We think it is more robust than a simple substraction. Indeed, a substraction between two small rates will be small even if the jobseeking rate is two times bigger than the unemployment rate. Here we really know how the two rates compares to each others.\n",
    "\n",
    "We decided to change the colors of the plot because it is not 'bad' nor 'good' to have employed people looking for a new job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we compute the variation rate\n",
    "df_ch['diff'] = (jobseekers_ratio_df['jobseeking rate'] - df_ch['unemployment rate'])/df_ch['unemployment rate']\n",
    "map_ch3 = folium.Map([46.9, 8.2],  tiles='cartodbpositron', zoom_start=8)\n",
    "map_ch3.choropleth(\n",
    "    geo_data=topo_ch_data,\n",
    "    name='choropleth',\n",
    "    data=df_ch,\n",
    "    columns=['cantons', 'diff'],\n",
    "    key_on='feature.properties.name',\n",
    "    topojson='objects.cantons',\n",
    "    fill_color='YlGnBu',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Variation rate between Jobseeking and Unemployment (%)',\n",
    "    threshold_scale=compute_breaks(df_ch['diff'])\n",
    ")\n",
    "map_ch3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Swiss and foreign workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(PATH_TASK_3_1)\n",
    "df_3.drop(index=[0,1,2,3], inplace=True)\n",
    "df_3.rename(columns={'2 Arbeitslosenquoten': 'canton'}, inplace=True)\n",
    "df_3.set_index('canton', drop=True, inplace=True)\n",
    "df_3.drop(df_3.columns[1 -1], axis=1, inplace=True)\n",
    "df_3.drop(columns='Unnamed: 20', inplace=True)\n",
    "df_3.drop(columns='Unnamed: 21', inplace=True)\n",
    "#column_names = df_3.loc[2]\n",
    "#df_3.columns = column_names\n",
    "\n",
    "# cast dataframe values as float\n",
    "df_3=df_3.astype(float)\n",
    "\n",
    "#df_3.drop(index=[0,1], axis=0, inplace=True)\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['januar'] = df_3['Unnamed: 2'].sub(df_3['Unnamed: 3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference(df_3, nb_months=12):\n",
    "    tmp = []\n",
    "    for i in range(1,nb_months):\n",
    "        colnme = np.str(i)\n",
    "        tmp.append(df_3.iloc[:, 2*i].sub(df_3.iloc[:, 2*i+1]))\n",
    "\n",
    "    tmp = np.array(tmp)\n",
    "    df_3['annual_rate_diff'] = np.mean(tmp, axis=0)\n",
    "    return df_3\n",
    "\n",
    "def compute_variation(df_3, nb_months=12):\n",
    "    tmp = []\n",
    "    for i in range(1,nb_months):\n",
    "        colnme = np.str(i)\n",
    "        tmp.append( (df_3.iloc[:, 2*i] - (df_3.iloc[:, 2*i+1]))/ df_3.iloc[:, 2*i+1])\n",
    "        \n",
    "    tmp = np.array(tmp)\n",
    "    df_3['variation_rate'] = np.mean(tmp, axis=0)\n",
    "    return df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = compute_difference(df_3, nb_months=9)\n",
    "new_df = compute_variation(df_3, nb_months=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this list, print each canton that does not appear in the DataFrame\n",
    "missing_cantons = set(cantons_topo).difference(set(new_df.index.values))\n",
    "print(\"Some names are not matching %d / %d\" %(len(missing_cantons), len(cantons_topo)))\n",
    "print('\\n'.join(missing_cantons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a few names for the cantons whose names are not matching between topojson and dataframe\n",
    "new_df = new_df.rename(index={'Genf':'Genève', 'Waadt':'Vaud', 'Neuenburg':'Neuchâtel', 'Graubünden':'Graubünden/Grigioni', 'Tessin':'Ticino', 'Bern':'Bern/Berne', 'Wallis':'Valais/Wallis','Freiburg':'Fribourg'})\n",
    "\n",
    "# assert that every name is matching\n",
    "assert(set(cantons_topo) == set(new_df.index.values))\n",
    "\n",
    "# we need a 'cantons' column to bind the data with folium\n",
    "new_df['cantons'] = new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_3 = folium.Map([46.9, 8.2],  tiles='cartodbpositron', zoom_start=8)\n",
    "map_3.choropleth(\n",
    "    geo_data=topo_ch_data,\n",
    "    name='choropleth',\n",
    "    data=new_df,\n",
    "    columns=['cantons', 'annual_rate_diff'],\n",
    "    key_on='feature.properties.name',\n",
    "    topojson='objects.cantons',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Difference between national or foreigner in Unemployment Rate (%)',\n",
    "    threshold_scale=compute_breaks(new_df['annual_rate_diff'])\n",
    ")\n",
    "map_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_4 = folium.Map([46.9, 8.2],  tiles='cartodbpositron', zoom_start=8)\n",
    "map_4.choropleth(\n",
    "    geo_data=topo_ch_data,\n",
    "    name='choropleth',\n",
    "    data=new_df,\n",
    "    columns=['cantons', 'variation_rate'],\n",
    "    key_on='feature.properties.name',\n",
    "    topojson='objects.cantons',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=1,\n",
    "    legend_name='Variation rate between national or foreigner in Unemployment Rate (%)',\n",
    "    threshold_scale=compute_breaks(new_df['variation_rate'])\n",
    ")\n",
    "map_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**TODO** : Where are the differences most visible ? Why do you think that is ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: part 2\n",
    "#### Add the differences between age groups \n",
    "Let's refine the analysis by adding the differences between age groups. After searching deep down in Amstat's website, we finally found a csv age/nationality separated, containing the unemployment **rates** (and not only the numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import and clean the dataframe\n",
    "df = pd.read_csv(PATH_TASK_3_2, index_col=[0,1,3]).drop([\"Age_category\"], axis=1)\n",
    "# We create a multiIndex to make the DataFrame cleaner and more readable\n",
    "df = df.unstack(level=[1,2])\n",
    "del df.index.name\n",
    "df.columns = df.columns.droplevel(0)\n",
    "df.replace(to_replace='...', value=np.NaN, inplace=True) # fill missing values with NaNs\n",
    "df = df.replace(to_replace={\"'\":\"\"}, regex=True).apply(pd.to_numeric)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = df.plot(kind='bar',\n",
    "               stacked=False,\n",
    "               figsize=(25, 12),\n",
    "               color=['#99d8c9', '#41ae76', '#005824', '#ef6548', '#fdbb84', '#990000'],\n",
    "               fontsize=15,\n",
    "               legend=True,\n",
    "              )\n",
    "axes.set_xlabel(\"Canton\", fontsize=20)\n",
    "axes.set_ylabel(\"Unemployment rate (% of active pop.)\", fontsize=20)\n",
    "axes.set_title(\"Unemployment rate by canton, age category and nationality\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
