{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas.\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# ignore annoying warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "### field names\n",
    "- The fields \"National\" in SL and Liberia mean the same as the \"Totals\" field in Guinea, i.e. the Total number of cases in the country.\n",
    "\n",
    "### missing data\n",
    "- When there's a NaN (the value was not filled), we assume that the data was lost, or not written down.\n",
    "- When a report (csv file) for a given day is missing, we assume that nothing happened during this day.\n",
    "- We are only interested in the Totals/National columns. If a row contains a NaN in this column, we chose to discard the row. This should not be problematic (if the number of NaNs in *Totals* is reasonably low), since we consider the daily **average** per month.\n",
    "\n",
    "### fields meaning and aggregation\n",
    "- Since we don't have additionnal information on the data, especially about the meanings of some fields, we decide to compute the new cases separaterly for the confirmed, probable and suspect cases. Same goes for the deaths.\n",
    "    - We also add a column that sums all the new cases/deaths\n",
    "\n",
    "\n",
    "### Used/Discarded fields\n",
    "- We only consider the totals/national fields, not the values per district/region. Sometimes, the sum of the values per region is a bit higher than the total. We assume that's because some smaller regions are not shown in the data.\n",
    "    - For this reason, if the *Totals* value is NaN, we cannot just sum up the values of the different regions.\n",
    "- After observing the data, we can safely assume that the deaths, unlike the new cases, are reported cumulatively. I.e., the number of deaths reported on day *d* is the number of deaths that happened this day, plus all the deaths that happened in the previous days. We did not use the *new deaths* values, because for some countries there is no distinction between the different cases (confirmed, probable and suspected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Process\n",
    "- Importing the data\n",
    "- Merge and clean the full dataset\n",
    "- Unstack Description column\n",
    "- Daily averages computation\n",
    "- Results\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data from CSVs\n",
    "guinea = []\n",
    "liberia = []\n",
    "sl = []\n",
    "dfs = [guinea, liberia, sl]\n",
    "i = 0\n",
    "folders = [\"guinea_data\", \"liberia_data\", \"sl_data\"]\n",
    "for folder in folders:\n",
    "    path = DATA_FOLDER + \"/ebola/\"\n",
    "    for file in os.listdir(path + folder):\n",
    "        dfs[i].append(pd.read_csv(path + folder + '/' + file))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Guinea data into DataFrame\n",
    "guinea = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[0]:\n",
    "    df['Country'] = 'Guinea' # set the new country column\n",
    "    dfs[0][i] = df\n",
    "    i += 1\n",
    "guinea = pd.concat(dfs[0])\n",
    "guinea.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberia data into DataFrame\n",
    "liberia = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[1]:\n",
    "    df['Country'] = 'Liberia'\n",
    "    df = df.rename(columns = {'National':'Totals', 'Variable':'Description'})\n",
    "    dfs[1][i] = df\n",
    "    i += 1\n",
    "liberia = pd.concat(dfs[1])\n",
    "liberia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sierra Leone data into DataFrame\n",
    "sl = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[2]:\n",
    "    df['Country'] = 'Sierra Leone'\n",
    "    df = df.rename(columns = {'National':'Totals', 'date' : 'Date', 'variable':'Description'})\n",
    "    dfs[2][i] = df\n",
    "    i += 1\n",
    "sl = pd.concat(dfs[2])\n",
    "sl.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge and clean the full dataset (all countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "full_data = pd.concat([guinea, liberia, sl])\n",
    "\n",
    "# remove each row whose 'Totals' value is NaN\n",
    "n_col_before = full_data.shape[0]\n",
    "full_data = full_data[full_data.Totals.notnull()]\n",
    "n_col_after = full_data.shape[0]\n",
    "print(\"Removed \" + str(n_col_before-n_col_after) + \" rows out of \" + str(n_col_before))\n",
    "\n",
    "# Specify that the Date column contains datetime objects\n",
    "full_data.Date = pd.to_datetime(full_data.Date)\n",
    "\n",
    "# To facilitate readability, we discard all the columns we don't need\n",
    "full_data_crop = full_data[['Country', 'Description', 'Totals', 'Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the dataframe that we'll use to compute the different values looks like so far: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " full_data_crop.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unstack *Description* column\n",
    "\n",
    "Each distinct value of the *Description* column becomes a new column. In order to do that, we first clean the different *Description* values so that there are consistent across the different countries.\n",
    "\n",
    "For example, in Sierra Leone, The number of confirmed deaths is referred as *death_confirmed*, but as *Total death/s in confirmed cases* in Liberia.\n",
    "\n",
    "Then, since we are only interested in deaths and new cases, we discard all the rows that concern other things.\n",
    "\n",
    "Finally, we noticed when trying to unstack the data that there were three exact duplicate rows from the liberian data. Thus, we removed those rows.\n",
    "\n",
    "We can then finally unstack the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names and regexes that we use to process the Description\n",
    "\n",
    "interests_rgx = ['(new_suspected|New cases of suspects|New Case\\/s \\(Suspected\\))',\n",
    "             '(new_probable|New cases of probables|New Case\\/s \\(Probable\\))',\n",
    "             '(new_confirmed|New cases of confirmed|New case\\/s \\(confirmed\\))',\n",
    "             '(death_suspected|Total deaths of suspects|Total death\\/s in suspected cases)',\n",
    "             '(death_probable|Total deaths of probables|Total death\\/s in probable cases)',\n",
    "             '(death_confirmed|Total deaths of confirmed|Total death\\/s in confirmed cases)'\n",
    "            ]\n",
    "\n",
    "interests_new_cases = ['New cases (suspected)', 'New cases (probable)', 'New cases (confirmed)']\n",
    "interests_death = ['Deaths in suspected cases', 'Deaths in probable cases', 'Deaths in confirmed cases']\n",
    "interests_names = interests_new_cases + interests_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We decided to unstack the Descriptions column in other columns\n",
    "\n",
    "# uniformize description in needed columns\n",
    "for i in range(len(interests_rgx)):\n",
    "    full_data_crop.Description = full_data_crop.Description.str.replace(interests_rgx[i], interests_names[i])\n",
    "\n",
    "# remove un-needed rows based on Description column\n",
    "_filter = full_data_crop.Description.isin(interests_names)\n",
    "full_data_crop = full_data_crop[_filter]\n",
    "\n",
    "# unstack data\n",
    "full_data_crop.index = pd.MultiIndex.from_arrays([full_data_crop.Description.values, full_data_crop.Country.values, full_data_crop.Date.values])\n",
    "\n",
    "# We discovered that there are 3 duplicated lines in the file 2014-10-04 from liberia\n",
    "# We then had to delete the duplicated rows from the data so that we could unstack it\n",
    "full_data_crop = full_data_crop[~full_data_crop.index.duplicated(keep='first')]\n",
    "full_data_crop = full_data_crop.drop(['Description', 'Date', 'Country'], axis=1)\n",
    "\n",
    "# unstack the Descriptions and remove unneccessary index level\n",
    "full_data_crop = full_data_crop.unstack(level=0)\n",
    "full_data_crop.columns = full_data_crop.columns.droplevel()\n",
    "\n",
    "# convert fields to numeric values\n",
    "full_data_crop = full_data_crop.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the data looks like at this point. We have a single dataframe, containing the 3 countries' data, from which we can compute the daily averages per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_data_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Daily averages computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined two functions that, given the country name and names of columns we are interested in, return the daily average number of new cases (resp. deaths) per month in that country.\n",
    "\n",
    "Since we have no further information about the meanings of *probable*, *suspected* and *confirmed*, we chose not to aggregate those values and compute a daily average for each of them.\n",
    "\n",
    "Since we assumed deaths to be cumulative (see *Assumptions*), we had to find a different way to compute the average deaths. In order to do so, for each report we compute the difference between its *deaths* value and the *deaths* value of the preceding report. Then we compute the daily average per month from this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a df containing the mean number of new cases for the country given in argument, \n",
    "# respectively for confirmed, probable and suspected new cases\n",
    "def compute_new_cases(country, col_names):\n",
    "    # take only the data for the country given in argument, and group the values by month\n",
    "    country_data = full_data_crop[full_data_crop.Country == country]\n",
    "    country_data = country_data.groupby(country_data.Date.dt.month)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for name in col_names:\n",
    "        df[name] = country_data[name].mean()\n",
    "    df['Country'] = country\n",
    "    df = df.set_index(['Country'], append=True)\n",
    "    df = df.swaplevel(0, 1)\n",
    "    return df\n",
    "\n",
    "# Same thing as the previous function, but for the number of deaths\n",
    "def compute_deaths(country, col_names):\n",
    "    # take only the data for the country given in argument, and group the values by month\n",
    "    country_data = full_data_crop[full_data_crop.Country == country]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for name in col_names:\n",
    "        death_values = country_data[name].subtract(country_data[name].shift(1))\n",
    "        country_data[name] = death_values\n",
    "    \n",
    "    country_data = country_data.groupby(country_data.Date.dt.month)\n",
    "    \n",
    "    for name in col_names:\n",
    "        df[name] = country_data[name].mean()\n",
    "        #df[name] = (np.max(country_data[name]) - np.min(country_data[name])).divide((np.max(country_data['Date']) - np.min(country_data['Date'])).dt.days)\n",
    "    df['Country'] = country\n",
    "    df = df.set_index(['Country'], append=True)\n",
    "    df = df.swaplevel(0, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# put Date and Country back as columns (from the indexes) to make groupbys cleaner\n",
    "full_data_crop['Date'] = full_data_crop.index.get_level_values(1).values\n",
    "full_data_crop['Country'] = full_data_crop.index.get_level_values(0).values\n",
    "\n",
    "guinea_new_cases = compute_new_cases('Guinea', interests_new_cases)\n",
    "guinea_deaths = compute_deaths('Guinea', interests_death)\n",
    "\n",
    "liberia_new_cases = compute_new_cases('Liberia', interests_new_cases)\n",
    "liberia_deaths = compute_deaths('Liberia', interests_death)\n",
    "\n",
    "sl_new_cases = compute_new_cases('Sierra Leone', interests_new_cases)\n",
    "sl_deaths = compute_deaths('Sierra Leone', interests_death)\n",
    "\n",
    "new_cases_daily_means = pd.concat([guinea_new_cases, liberia_new_cases, sl_new_cases])\n",
    "new_cases_daily_means['Total new cases'] = new_cases_daily_means.sum(axis=1)\n",
    "deaths_daily_means = pd.concat([guinea_deaths, liberia_deaths, sl_deaths])\n",
    "deaths_daily_means['Total deaths'] = deaths_daily_means.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the daily average per month of new cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_daily_means.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the daily average per month of deaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deaths_daily_means.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN values\n",
    "There are NaN values at some places in the results. This happens when there is not a single data sample for an entire month. We decided to leave these NaN values as is, as we don't have enough data to interpolate missing values. We just replaced the NaNs by \"-\" to make it clear that there is no data here.\n",
    "\n",
    "#### Outliers\n",
    "The only big outlier values we have are for Liberia (new cases) during the month of december. We suspect that at some point during the month, the reports became cumulative for some reason. Since we have no way to confirm it, we decided to leave these values as they are, although we could have processed them as cumulative values (as we did it for deaths).\n",
    "\n",
    "#### Cumulative values decreasing\n",
    "Sometimes, even though the values are cumulative, we noticed that some values can decrease over time. After some research on the internet, we found that it could be because of some mistakes that were corrected (people we thought were dead but actually no)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "path = DATA_FOLDER + \"/microbiome/\"\n",
    "for file in os.listdir(path):\n",
    "    if file != 'metadata.xls' :\n",
    "        dfs.append(pd.read_excel(path + '/' + file , header = None , index_col = 0))\n",
    "    \n",
    "Meta = pd.read_excel(\"Data/microbiome/metadata.xls\", index_col = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta['SAMPLE'].fillna('unknown' , inplace = True )\n",
    "arrays = [Meta['GROUP'].values, Meta['SAMPLE'].values]\n",
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge = pd.concat(dfs , axis = 1)\n",
    "merge.columns = arrays\n",
    "merge.fillna('unknown' , inplace = True )\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge.shape)\n",
    "merge.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob, os  \n",
    "\n",
    "df3 = pd.read_excel(DATA_FOLDER+'/titanic.xls')\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describing the type of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information about the attributes of our dataset, we will set to caterogical (sex,cabin,embarked,boat and home.dest) features to categorical. Body, ticket and name normally being different for all passengers are not converted to categorical. \n",
    "\n",
    "There are 295 cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(df3.groupby('name').count()))\n",
    "df3.groupby('name').count()\n",
    "a = df3.index[df3.name.isnull() == True]\n",
    "df3.loc[a]\n",
    "print((df3.name.value_counts()>1)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are two people without names. when trying to see their attributes we see no rows. Maybe some people have their name written twice. Those people are Kelly, Mr. James and Connolly, Miss. Kate. Looking at their attributes, we will remove the versions with the least amount of data, loc[726,925]. We could also have decided to keep the four rows or to delete two of them in function of the fare they paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.name.isin(['Connolly, Miss. Kate','Kelly, Mr. James'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.index[[726,925]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert to categorical\n",
    "df3[\"sex\"] = pd.Categorical(df3['sex'], df3['sex'].unique())\n",
    "df3['cabin'] = df3['cabin'].fillna('missing')\n",
    "df3[\"cabin\"] = pd.Categorical(df3['cabin'], df3['cabin'].unique())\n",
    "df3['embarked'] = df3['embarked'].fillna('missing')\n",
    "df3[\"embarked\"] = pd.Categorical(df3['embarked'], df3['embarked'].unique())\n",
    "df3['boat'] = df3['boat'].fillna('missing')\n",
    "df3[\"boat\"] = pd.Categorical(df3['boat'], df3['boat'].unique())\n",
    "df3['home.dest'] = df3['home.dest'].fillna('missing')\n",
    "df3[\"home.dest\"] = pd.Categorical(df3['home.dest'], df3['home.dest'].unique())\n",
    "\n",
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "talk about range and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now all the missing values have been cleaned except for the body column. Since there are only 121 entries out of 1309, we will see later if we remove this column.\n",
    "\n",
    "### 2. Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "fig.set_size_inches(20,20)\n",
    "\n",
    "df_temporary = df3.copy()\n",
    "df_temporary.age = pd.cut(df3.age, [0,10,20,30,40,50,60,70,80,90])\n",
    "sns.countplot(x='age',data=df_temporary,ax=axes[0,0])\n",
    "axes[0,0].set_xlabel('age')\n",
    "axes[0,0].set_ylabel('count')\n",
    "\n",
    "classes = df3.groupby('pclass').size()\n",
    "#df3.hist('pclass',ax=axes[0,1])\n",
    "classes.plot(kind='bar',ax=axes[0,1])\n",
    "axes[0,1].set_xlabel('class')\n",
    "axes[0,1].set_ylabel('count')\n",
    "\n",
    "sns.countplot(x='sex',data=df3,ax=axes[1,0])\n",
    "\n",
    "sns.countplot(x='embarked',data=df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram observations\n",
    "- We can observe the passenger age distribution in the titanic is gaussian\n",
    "- There is a higher amount of males onboard\n",
    "- There were more people embarked in third class then first and second combined\n",
    "- Most of the passengers boarded in Southampton\n",
    "\n",
    "### 3. Calculate the proportion of passengers by cabin floor\n",
    "We make the assumption that for cabins with name as 'F E69' belong to the cabin floor F. It should be noted that the following pie chart does not represent perfectly the true proportion of passengers per cabin floor as only 295 out of 1309 values of the cabin column are filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3['cabin_floor'] = df3.cabin.apply(lambda x: x[0])\n",
    "df3.cabin_floor = df3.cabin_floor[df3.cabin_floor != 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.pie(df3.groupby('cabin_floor').count())\n",
    "import plotly as ip\n",
    "#plt.pie(df3.groupby('cabin_floor').count())\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "#ip.tools.set_credentials_file(username='USERNAME', api_key='KEY')\n",
    "#trace = go.Pie(labels=df3['cabin_floor'].value_counts().index, values=df3['cabin_floor'].value_counts())\n",
    "#py.iplot([trace], filename='Proportion of passengers per cabin floor')\n",
    "\n",
    "#Image(url= \"https://plot.ly/~mgelsm/2.embed\")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://plot.ly/~mgelsm/2', width=800, height=600)\n",
    "#![Image of Yaktocat](https://plot.ly/~mgelsm/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is plotted in case the viewer does not run the boxes. The interactive plot is however not available below.\n",
    "<div>\n",
    "    <a href=\"https://plot.ly/~mgelsm/2/?share_key=LTW3TPD50iXkdYukf55G9j\" target=\"_blank\" title=\"Proportion of passengers per cabin floor\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~mgelsm/2.png?share_key=LTW3TPD50iXkdYukf55G9j\" alt=\"Proportion of passengers per cabin floor\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https://plot.ly/404.png';\" /></a>\n",
    "    <script data-plotly=\"mgelsm:2\" sharekey-plotly=\"LTW3TPD50iXkdYukf55G9j\" src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. For each travel class, calculate the proportion of the passengers that survived. Present your results in pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ip.tools.make_subplots(rows=3, cols=1)\n",
    "trace1 = go.Pie(labels=df3.loc[df3.pclass == 1].survived.value_counts().index, values=df3.loc[df3.pclass == 1].survived.value_counts())\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Pie(labels=df3.loc[df3.pclass == 2].survived.value_counts().index, values=df3.loc[df3.pclass == 2].survived.value_counts())\n",
    "fig.append_trace(trace2, 2, 1)\n",
    "trace1 = go.Pie(labels=df3.loc[df3.pclass == 3].survived.value_counts().index, values=df3.loc[df3.pclass == 1].survived.value_counts())\n",
    "fig.append_trace(trace3, 3, 1)\n",
    "py.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate the proportion of the passengers that survived by travel class and sex. Present your results in a single histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x='sex',y='survived',hue='pclass',data=df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that more women survived from the titanic sinking and upper classes have a higher rate of survival.\n",
    "\n",
    "### 6. Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. Present your results in a DataFrame with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['age_class'] = df3.age < df3.age.median()\n",
    "df3.age.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_category(x):\n",
    "    if x.age_class == False and x.sex == 'female':\n",
    "        val = 'young_female'\n",
    "    elif x.age_class == True and x.sex == 'female':\n",
    "        val = 'old_female'\n",
    "    elif x.age_class == False and x.sex == 'male':\n",
    "        val = 'young_male'\n",
    "    else:\n",
    "        val = 'old_male'   \n",
    "    return val\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3['age_sex'] = df3.apply(create_category,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='age_sex',y='survived',hue='pclass',data=df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This histogram shows two important things :\n",
    "1. There is a higher rate of Women and children that survived\n",
    "2. People in hihger classes had better chances of surviving\n",
    "\n",
    "Hence we can understand that the rule of saving children and women first was applied during the titanic sinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
