{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- We only consider the totals/national fields, not the values per district/region. Sometimes, the sum of the values per region is a bit higher than the total. We assume that's because some smaller regions are not shown in the data.\n",
    "    - For this reason, if the *Totals* value is NaN, we cannot just sum up the values of the different regions.\n",
    "- The fields \"National\" in SL and Liberia mean the same as the \"Totals\" field in Guinea, i.e. the Total number of cases in the country.\n",
    "- When there's a NaN (the value was not filled), we assume that the data was lost, or not written down.\n",
    "- We are only interested in the Totals/National columns. If a row contains a NaN in this column, we chose to discard the row. This should not be problematic (if the number of NaNs in *Totals* is reasonably low), since we consider the daily **average** per month.\n",
    "- Since we don't have additionnal information on the data, especially about the meanings of some fields, we decide to compute the new cases separaterly for the confirmed, probable and suspect cases. Same goes for the deaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "- first, we import the data and store it separately for each country\n",
    "- then we rename the fields that we will use (Totals, Description,...) so that they are the same for each country\n",
    "- we also add two new fields \"Country\" and \"Month\" (derived from Date) in each of the 3 dataframes, to simplify the process for later\n",
    "- We then concatenate the 3 dataframes into a single dataframe, called *full_data*\n",
    "- Then, to facilitate readability we crop this dataframe (keep only the columns *month*, *Description*, *Totals*, *Country*) and store the result in *full_data_crop*\n",
    "- We do a couple of sanity checkups, to ensure that the data is well-formated and not erroneous (1 <= month <= 12, no NaN value, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "guinea = []\n",
    "liberia = []\n",
    "sl = []\n",
    "dfs = [guinea, liberia, sl]\n",
    "i = 0\n",
    "folders = [\"guinea_data\", \"liberia_data\", \"sl_data\"]\n",
    "for folder in folders:\n",
    "    path = DATA_FOLDER + \"/ebola/\"\n",
    "    for file in os.listdir(path + folder):\n",
    "        dfs[i].append(pd.read_csv(path + folder + '/' + file))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Guinea data into DataFrame\n",
    "guinea = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[0]:\n",
    "    df['Country'] = 'Guinea' # set the new country column\n",
    "    dfs[0][i] = df\n",
    "    i += 1\n",
    "guinea = pd.concat(dfs[0])\n",
    "guinea.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberia data into DataFrame\n",
    "liberia = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[1]:\n",
    "    df['Country'] = 'Liberia'\n",
    "    df = df.rename(columns = {'National':'Totals', 'Variable':'Description'})\n",
    "    dfs[1][i] = df\n",
    "    i += 1\n",
    "liberia = pd.concat(dfs[1])\n",
    "liberia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sierra Leone data into DataFrame\n",
    "sl = pd.DataFrame()\n",
    "i = 0\n",
    "for df in dfs[2]:\n",
    "    df['Country'] = 'Sierra Leone'\n",
    "    df = df.rename(columns = {'National':'Totals', 'date' : 'Date', 'variable':'Description'})\n",
    "    dfs[2][i] = df\n",
    "    i += 1\n",
    "sl = pd.concat(dfs[2])\n",
    "sl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "full_data = pd.concat([guinea, liberia, sl])\n",
    "\n",
    "# remove each row whose 'Totals' value is NaN\n",
    "n_col_before = full_data.shape[0]\n",
    "full_data = full_data[full_data.Totals.notnull()]\n",
    "n_col_after = full_data.shape[0]\n",
    "print(\"Removed \" + str(n_col_before-n_col_after) + \" rows out of \" + str(n_col_before))\n",
    "\n",
    "# Specify that the Date column contains datetime objects\n",
    "full_data.Date = pd.to_datetime(full_data.Date)\n",
    "\n",
    "# To facilitate readability, we discard all the columns we don't need\n",
    "full_data_crop = full_data[['Country', 'Description', 'Totals', 'Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the dataframe that we'll use to compute the different values looks like so far\n",
    "full_data_crop.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names and regexes that we use to process the Description\n",
    "\n",
    "interests_rgx = ['(new_suspected|New cases of suspects|New Case\\/s \\(Suspected\\))',\n",
    "             '(new_probable|New cases of probables|New Case\\/s \\(Probable\\))',\n",
    "             '(new_confirmed|New cases of confirmed|New case\\/s \\(confirmed\\))',\n",
    "             '(death_suspected|Total deaths of suspects|Total death\\/s in suspected cases)',\n",
    "             '(death_probable|Total deaths of probables|Total death\\/s in probable cases)',\n",
    "             '(death_confirmed|Total deaths of confirmed|Total death\\/s in confirmed cases)'\n",
    "            ]\n",
    "\n",
    "interests_new_cases = ['New cases (suspected)', 'New cases (probable)', 'New cases (confirmed)']\n",
    "interests_death = ['Deaths in suspected cases', 'Deaths in probable cases', 'Deaths in confirmed cases']\n",
    "interests_names = interests_new_cases + interests_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to unstack the Descriptions column in other columns\n",
    "\n",
    "# uniformize description in needed columns\n",
    "for i in range(len(interests_rgx)):\n",
    "    full_data_crop.Description = full_data_crop.Description.str.replace(interests_rgx[i], interests_names[i])\n",
    "\n",
    "# remove un-needed rows based on Description column\n",
    "_filter = full_data_crop.Description.isin(interests_names)\n",
    "full_data_crop = full_data_crop[_filter]\n",
    "\n",
    "# unstack data\n",
    "full_data_crop.index = pd.MultiIndex.from_arrays([full_data_crop.Description.values, full_data_crop.Country.values, full_data_crop.Date.values])\n",
    "\n",
    "# We discovered that there are 3 duplicated lines in the file 2014-10-04 from liberia\n",
    "# We then had to delete the duplicated rows from the data so that we could unstack it\n",
    "full_data_crop = full_data_crop[~full_data_crop.index.duplicated(keep='first')]\n",
    "full_data_crop = full_data_crop.drop(['Description', 'Date', 'Country'], axis=1)\n",
    "\n",
    "# unstack the Descriptions and remove unneccessary index level\n",
    "full_data_crop = full_data_crop.unstack(level=0)\n",
    "full_data_crop.columns = full_data_crop.columns.droplevel()\n",
    "\n",
    "# convert fields to numeric values\n",
    "full_data_crop = full_data_crop.apply(pd.to_numeric)\n",
    "\n",
    "full_data_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# put Date and Country back as columns (from the indexes) to make groupbys cleaner\n",
    "full_data_crop['Date'] = full_data_crop.index.get_level_values(1).values\n",
    "full_data_crop['Country'] = full_data_crop.index.get_level_values(0).values\n",
    "\n",
    "x = full_data_crop[full_data_crop.Country == 'Guinea']\n",
    "y = x.groupby(x.Date.dt.month)\n",
    "Guinea_Confirmed = (np.max(y['Deaths in confirmed cases']) - np.min(y['Deaths in confirmed cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "Guinea_Probable = (np.max(y['Deaths in probable cases']) - np.min(y['Deaths in probable cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "Guinea_Suspected = (np.max(y['Deaths in suspected cases']) - np.min(y['Deaths in suspected cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "Guinea_New_Confirmed = y['New cases (confirmed)'].mean()\n",
    "Guinea_New_Probable = y['New cases (probable)'].mean()\n",
    "Guinea_New_Suspected = y['New cases (suspected)'].mean()\n",
    "print('Daily average deaths for Guinea')\n",
    "print(Guinea_Confirmed)\n",
    "print(Guinea_Probable)\n",
    "print(Guinea_Suspected)\n",
    "print(Guinea_New_Confirmed)\n",
    "print(Guinea_New_Probable)\n",
    "print(Guinea_New_Suspected)\n",
    "\n",
    "\n",
    "x = full_data_crop[full_data_crop.Country == 'Liberia']\n",
    "y = x.groupby(x.Date.dt.month)\n",
    "Liberia_Confirmed = (np.max(y['Deaths in confirmed cases']) - np.min(y['Deaths in confirmed cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "Liberia_Probable = (np.max(y['Deaths in probable cases']) - np.min(y['Deaths in probable cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "Liberia_Suspected = (np.max(y['Deaths in suspected cases']) - np.min(y['Deaths in suspected cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "print('Daily average deaths for Liberia')\n",
    "print(Liberia_Confirmed)\n",
    "print(Liberia_Probable)\n",
    "print(Liberia_Suspected)\n",
    "\n",
    "x = full_data_crop[full_data_crop.Country == 'Sierra Leone']\n",
    "y = x.groupby(x.Date.dt.month)\n",
    "SL_Confirmed = (np.max(y['Deaths in confirmed cases']) - np.min(y['Deaths in confirmed cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "SL_Probable = (np.max(y['Deaths in probable cases']) - np.min(y['Deaths in probable cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "SL_Suspected = (np.max(y['Deaths in suspected cases']) - np.min(y['Deaths in suspected cases'])).divide((np.max(y['Date']) - np.min(y['Date'])).dt.days)\n",
    "print('Daily average deaths for Sierra Leone')\n",
    "print(SL_Confirmed)\n",
    "print(SL_Probable)\n",
    "print(SL_Suspected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we assume that the deaths are cumulative (everyday, you get the number of deaths so far, including the deaths that occured\n",
    "# during previous days). Thus we proceed as follows: \n",
    "# - take the max and min number of death for each <Month, Country> pair, take their different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remarks\n",
    "\n",
    "TODO deaths are cumulative, can clone column, shift it and substract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test shift\n",
    "d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),\n",
    "   'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}\n",
    "df = pd.DataFrame(d)\n",
    "print(df)\n",
    "df = df.two.shift(1)\n",
    "print(df)\n",
    "assert(df.isnull().sum()==1)\n",
    "df = df.fillna(0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the desired values\n",
    "\n",
    "interests = ['(new_suspected|New cases of suspects|New Case\\/s \\(Suspected\\))',\n",
    "             '(new_probable|New cases of probables|New Case\\/s \\(Probable\\))',\n",
    "             '(new_confirmed|New cases of confirmed|New case\\/s \\(confirmed\\))',\n",
    "             '(death_suspected|Total deaths of suspects|Total death\\/s in confirmed cases)',\n",
    "             '(death_probable|Total deaths of probables|Total death\\/s in probable cases)',\n",
    "             '(death_confirmed|Total deaths of confirmed|Total death\\/s in confirmed cases)'\n",
    "            ]\n",
    "death_interests = [\n",
    "            '(death_suspected|Total deaths of suspects|Total death\\/s in confirmed cases)',\n",
    "            '(death_probable|Total deaths of probables|Total death\\/s in probable cases)',\n",
    "            '(death_confirmed|Total deaths of confirmed|Total death\\/s in confirmed cases)'\n",
    "            ]\n",
    "all_means = []\n",
    "for interest in interests:\n",
    "    rows = full_data_crop[full_data_crop.Description.str.match(interest)]\n",
    "    #print(rows.shape)\n",
    "    rows.Totals = pd.to_numeric(rows.Totals)\n",
    "    if(interest in death_interests):\n",
    "        #print(type(rows))\n",
    "        total_deaths = rows.Totals.shift(1).fillna(0)\n",
    "        print('before: ')\n",
    "        print(rows.Totals[1:10])\n",
    "        rows.Totals = rows.Totals - total_deaths\n",
    "        print('after: ')\n",
    "        print(rows.Totals[1:10])\n",
    "        \n",
    "    merge_index = [rows.Month, rows.Country]\n",
    "    means = rows.groupby(merge_index).agg(np.mean)\n",
    "    #means = means[['Totals']]\n",
    "    #assert((rows.Totals >= 0).all()) # assert that the Totals is always >= 0\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    n_values = rows.groupby(merge_index).agg(['count', np.mean])\n",
    "    #print(means)\n",
    "    #print(n_values['Totals'])\n",
    "    # /test\n",
    "\n",
    "    means.columns = [interest]\n",
    "    all_means.append(means)\n",
    "\n",
    "current = all_means[0]\n",
    "for mean in all_means[1:]:\n",
    "    current = pd.merge(current, mean,  left_index=True, right_index=True)\n",
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "path = DATA_FOLDER + \"/microbiome/\"\n",
    "for file in os.listdir(path):\n",
    "    if file != 'metadata.xls' :\n",
    "        dfs.append(pd.read_excel(path + '/' + file , header = None , index_col = 0))\n",
    "    \n",
    "Meta = pd.read_excel(\"Data/microbiome/metadata.xls\", index_col = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta['SAMPLE'].fillna('unknown' , inplace = True )\n",
    "arrays = [Meta['GROUP'].values, Meta['SAMPLE'].values]\n",
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge = pd.concat(dfs , axis = 1)\n",
    "merge.columns = arrays\n",
    "merge.fillna('unknown' , inplace = True )\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge.shape)\n",
    "merge.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
